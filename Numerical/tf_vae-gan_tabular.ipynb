{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 05:54:49.573991: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-05 05:54:49.735536: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736036689.865034    7518 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736036689.892849    7518 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-05 05:54:50.093975: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['mainroad', 'guestroom', 'basement', 'hotwaterheating',\n",
      "       'airconditioning', 'prefarea', 'furnishingstatus'],\n",
      "      dtype='object')\n",
      "Index(['furnishingstatus_furnished', 'furnishingstatus_semi-furnished',\n",
      "       'furnishingstatus_unfurnished'],\n",
      "      dtype='object')\n",
      "price                              int64\n",
      "area                               int64\n",
      "bedrooms                           int64\n",
      "bathrooms                          int64\n",
      "stories                            int64\n",
      "mainroad                           int64\n",
      "guestroom                          int64\n",
      "basement                           int64\n",
      "hotwaterheating                    int64\n",
      "airconditioning                    int64\n",
      "parking                            int64\n",
      "prefarea                           int64\n",
      "furnishingstatus_furnished         int64\n",
      "furnishingstatus_semi-furnished    int64\n",
      "furnishingstatus_unfurnished       int64\n",
      "dtype: object\n",
      "Index(['price', 'mainroad', 'guestroom', 'basement', 'hotwaterheating',\n",
      "       'airconditioning', 'prefarea', 'furnishingstatus_furnished',\n",
      "       'furnishingstatus_semi-furnished', 'furnishingstatus_unfurnished',\n",
      "       'area_standardized', 'bedrooms_standardized', 'bathrooms_standardized',\n",
      "       'stories_standardized', 'parking_standardized'],\n",
      "      dtype='object')\n",
      "   mainroad  guestroom  basement  hotwaterheating  airconditioning  prefarea  \\\n",
      "0         1          0         0                0                1         1   \n",
      "1         1          0         0                0                1         0   \n",
      "2         1          0         1                0                0         1   \n",
      "3         1          0         1                0                1         1   \n",
      "4         1          1         1                0                1         0   \n",
      "\n",
      "   furnishingstatus_furnished  furnishingstatus_semi-furnished  \\\n",
      "0                           1                                0   \n",
      "1                           1                                0   \n",
      "2                           0                                1   \n",
      "3                           1                                0   \n",
      "4                           1                                0   \n",
      "\n",
      "   furnishingstatus_unfurnished  area_standardized  bedrooms_standardized  \\\n",
      "0                             0           1.045766               1.402131   \n",
      "1                             0           1.755397               1.402131   \n",
      "2                             0           2.216196               0.047235   \n",
      "3                             0           1.082630               1.402131   \n",
      "4                             0           1.045766               1.402131   \n",
      "\n",
      "   bathrooms_standardized  stories_standardized  parking_standardized  \n",
      "0                1.420507              1.376952              1.516299  \n",
      "1                5.400847              2.529700              2.676950  \n",
      "2                1.420507              0.224204              1.516299  \n",
      "3                1.420507              0.224204              2.676950  \n",
      "4               -0.569663              0.224204              1.516299  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1736036694.237257    7518 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2171 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"/home/mw/.cache/kagglehub/datasets/yasserh/housing-prices-dataset/versions/1/Housing.csv\")\n",
    "data = data.dropna()\n",
    "\n",
    "# Handle categorical columns\n",
    "categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "print(categorical_columns)\n",
    "\n",
    "for col in categorical_columns:\n",
    "    if data[col].nunique() <= 2:\n",
    "        data[col] = LabelEncoder().fit_transform(data[col])\n",
    "    else:\n",
    "        one_hot = pd.get_dummies(data[col], prefix=col)\n",
    "        data = pd.concat([data.drop(col, axis=1), one_hot], axis=1)\n",
    "\n",
    "# Handle boolean columns\n",
    "boolean_columns = data.select_dtypes(include='bool').columns\n",
    "print(boolean_columns)\n",
    "data[boolean_columns] = data[boolean_columns].astype(int)\n",
    "print(data.dtypes)\n",
    "\n",
    "# Standardize numeric columns\n",
    "numeric_columns = data.select_dtypes(include=['float64']).columns\n",
    "data[numeric_columns] = (data[numeric_columns] - data[numeric_columns].mean()) / data[numeric_columns].std()\n",
    "\n",
    "# Standardize specific columns\n",
    "data['area_standardized'] = (data['area'] - data['area'].mean()) / data['area'].std()\n",
    "data['bedrooms_standardized'] = (data['bedrooms'] - data['bedrooms'].mean()) / data['bedrooms'].std()\n",
    "data['bathrooms_standardized'] = (data['bathrooms'] - data['bathrooms'].mean()) / data['bathrooms'].std()\n",
    "data['stories_standardized'] = (data['stories'] - data['stories'].mean()) / data['stories'].std()\n",
    "data['parking_standardized'] = (data['parking'] - data['parking'].mean()) / data['parking'].std()\n",
    "\n",
    "# Drop the original columns\n",
    "data = data.drop(columns=['area', 'bedrooms', 'bathrooms', 'stories', 'parking'])\n",
    "\n",
    "print(data.columns)\n",
    "\n",
    "# Define target and features\n",
    "target_column = \"price\"\n",
    "features = data.drop(target_column, axis=1)\n",
    "target = data[target_column]\n",
    "\n",
    "print(features.head())\n",
    "\n",
    "# Convert features and target to TensorFlow tensors\n",
    "features_tensor = tf.convert_to_tensor(features.values, dtype=tf.float32)\n",
    "target_tensor = tf.convert_to_tensor(target.values, dtype=tf.float32)\n",
    "\n",
    "# Create a TensorFlow Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features_tensor, target_tensor))\n",
    "dataset = dataset.shuffle(buffer_size=len(data)).batch(128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.src import layers, Model, optimizers\n",
    "import tensorflow as tf\n",
    "\n",
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = tf.keras.Sequential([\n",
    "            layers.Dense(256, input_dim=input_dim),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2),\n",
    "            layers.Dense(128),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2),\n",
    "            layers.Dense(1)  # Output layer for binary classification (real or fake)\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        features = x  # Start with the input as the initial features\n",
    "        for i, layer in enumerate(self.net.layers[:-1]):\n",
    "            features = layer(features)  # Apply each layer to the features\n",
    "        \n",
    "        logits = self.net.layers[-1](features)  # Apply the final Dense layer to get logits\n",
    "        output = tf.sigmoid(logits)  # Apply sigmoid to get the binary output (real or fake)\n",
    "        return output, features  # Return the output and the features (intermediate layer outputs)\n",
    "\n",
    "\n",
    "class Encoder(Model):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.fc1 = layers.Dense(512, input_dim=input_dim, activation='relu')\n",
    "        self.fc2 = layers.Dense(256, activation='relu')\n",
    "        self.fc3 = layers.Dense(128, activation='relu')\n",
    "        self.fc_mean = layers.Dense(latent_dim)\n",
    "        self.fc_log_var = layers.Dense(latent_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        print(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        mean = self.fc_mean(x)\n",
    "        log_var = self.fc_log_var(x)\n",
    "        return mean, log_var\n",
    "\n",
    "class Decoder(Model):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.fc1 = layers.Dense(128, input_dim=latent_dim, activation='relu')\n",
    "        self.fc2 = layers.Dense(256, activation='relu')\n",
    "        self.fc3 = layers.Dense(512, activation='relu')\n",
    "        self.fc_output = layers.Dense(output_dim, activation='sigmoid')\n",
    "\n",
    "    def call(self, z):\n",
    "        z = self.fc1(z)\n",
    "        z = self.fc2(z)\n",
    "        z = self.fc3(z)\n",
    "        reconstructed_x = self.fc_output(z)\n",
    "        return reconstructed_x\n",
    "\n",
    "class VAE_GAN(Model):\n",
    "    def __init__(self, encoder, decoder, discriminator):\n",
    "        super(VAE_GAN, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.discriminator = discriminator\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = tf.exp(0.5 * logvar)\n",
    "        eps = tf.random.normal(shape=tf.shape(std))\n",
    "        return mu + eps * std\n",
    "        \n",
    "    def call(self, x):\n",
    "        # Encode\n",
    "        print(x)\n",
    "        z_mean, z_log_var = self.encoder(x)\n",
    "        \n",
    "        # Reparameterize\n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "        # Decode\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        \n",
    "        # Discriminate both real and reconstructed\n",
    "        # real_or_fake_input = tf.concat([x, x_reconstructed], axis=-1)\n",
    "        real_or_fake_reconstructed, features_constructed = self.discriminator(x_reconstructed)\n",
    "        real_or_fake_real, features_real = self.discriminator(x)\n",
    "        \n",
    "        return (\n",
    "            x_reconstructed,\n",
    "            real_or_fake_reconstructed,\n",
    "            real_or_fake_real,\n",
    "            z_mean,\n",
    "            z_log_var\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae_gan(model, dataset, epochs=200, learning_rate=1e-3):\n",
    "    # Define optimizers\n",
    "    optimizer_enc = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    optimizer_dec = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    optimizer_dis = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    print('entered')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Trainable variables in the model:\")\n",
    "        for var in model.trainable_variables:\n",
    "            print(var.name)  \n",
    "            \n",
    "        # print(model.summary())# Prints the name of each trainable variable\n",
    "\n",
    "        running_vae_loss = 0\n",
    "        running_dis_loss = 0\n",
    "        running_gen_loss = 0\n",
    "        \n",
    "        for batch in tqdm(dataset, desc=f'Epoch {epoch + 1}/{epochs}'):\n",
    "            inputs = batch[0]  # Extract inputs (e.g., images)\n",
    "\n",
    "            # Train Discriminator\n",
    "            with tf.GradientTape() as tape_dis:\n",
    "                x_reconstructed, real_or_fake_reconstructed, real_or_fake_real, z_mean, z_log_var = model(inputs)\n",
    "                \n",
    "                # Labels for discriminator\n",
    "                real_labels = tf.ones_like(real_or_fake_real)\n",
    "                fake_labels = tf.zeros_like(real_or_fake_reconstructed)\n",
    "                \n",
    "                # Compute Discriminator Loss\n",
    "                dis_real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    logits=real_or_fake_real, labels=real_labels))\n",
    "                dis_fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    logits=real_or_fake_reconstructed, labels=fake_labels))\n",
    "                dis_loss = dis_real_loss + dis_fake_loss\n",
    "            \n",
    "            gradients_dis = tape_dis.gradient(dis_loss, model.discriminator.trainable_variables)\n",
    "            optimizer_dis.apply_gradients(zip(gradients_dis, model.discriminator.trainable_variables))\n",
    "            \n",
    "            # Train Generator (Decoder)\n",
    "            with tf.GradientTape() as tape_dec:\n",
    "                gen_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    logits=real_or_fake_reconstructed, labels=real_labels))\n",
    "            \n",
    "            gradients_dec = tape_dec.gradient(gen_loss, model.decoder.trainable_variables)\n",
    "            optimizer_dec.apply_gradients(zip(gradients_dec, model.decoder.trainable_variables))\n",
    "\n",
    "            # Train VAE (Encoder + Decoder)\n",
    "            with tf.GradientTape() as tape_vae:\n",
    "                # Reconstruction Loss\n",
    "                recon_loss = tf.reduce_mean(tf.square(x_reconstructed - inputs))\n",
    "                # KL-Divergence Loss\n",
    "                kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "                vae_loss = recon_loss + kl_loss\n",
    "            \n",
    "            gradients_enc = tape_vae.gradient(vae_loss, model.encoder.trainable_variables)\n",
    "            optimizer_enc.apply_gradients(zip(gradients_enc, model.encoder.trainable_variables))\n",
    "\n",
    "            # Track losses\n",
    "            running_dis_loss += dis_loss.numpy()\n",
    "            running_gen_loss += gen_loss.numpy()\n",
    "            running_vae_loss += vae_loss.numpy()\n",
    "\n",
    "        # Print average losses per epoch\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}]: D: {running_dis_loss / len(dataset):.4f}, G: {running_gen_loss / len(dataset):.4f}, VAE: {running_vae_loss / len(dataset):.4f}')\n",
    "\n",
    "        # Save model weights every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            model.save_weights(f'vae_gan_epoch_{epoch + 1}.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mw/miniconda3/envs/research/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.8728698   0.7514581  -1.9147625  ...  0.25894696 -1.1722629\n",
      "  -1.2254761 ]\n",
      " [-0.26744348 -0.15550892  0.01041336 ...  0.09169319  0.36868748\n",
      "   0.74312085]\n",
      " [-0.08855354  1.0123518  -0.93546706 ... -0.26214638 -1.4083754\n",
      "  -0.2827134 ]\n",
      " ...\n",
      " [ 0.6975861   0.19835013  0.7918325  ... -1.3251573  -0.5483585\n",
      "   0.8277462 ]\n",
      " [ 0.8718931   0.00763387 -0.6905358  ... -0.1721974   0.88182336\n",
      "   1.8044299 ]\n",
      " [-0.33327076  0.3534472   0.43091783 ...  0.05074676 -0.4394912\n",
      "  -1.2760217 ]], shape=(128, 14), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.8728698   0.7514581  -1.9147625  ...  0.25894696 -1.1722629\n",
      "  -1.2254761 ]\n",
      " [-0.26744348 -0.15550892  0.01041336 ...  0.09169319  0.36868748\n",
      "   0.74312085]\n",
      " [-0.08855354  1.0123518  -0.93546706 ... -0.26214638 -1.4083754\n",
      "  -0.2827134 ]\n",
      " ...\n",
      " [ 0.6975861   0.19835013  0.7918325  ... -1.3251573  -0.5483585\n",
      "   0.8277462 ]\n",
      " [ 0.8718931   0.00763387 -0.6905358  ... -0.1721974   0.88182336\n",
      "   1.8044299 ]\n",
      " [-0.33327076  0.3534472   0.43091783 ...  0.05074676 -0.4394912\n",
      "  -1.2760217 ]], shape=(128, 14), dtype=float32)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Encoder output shape: (128, 10)\n",
      "Decoder output shape: (128, 14)\n",
      "Discriminator output shape: (128, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"vae_gan\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"vae_gan\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)               │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">174,484</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)               │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">173,198</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ discriminator (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Discriminator</span>)   │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">38,401</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder (\u001b[38;5;33mEncoder\u001b[0m)               │ ?                      │       \u001b[38;5;34m174,484\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder (\u001b[38;5;33mDecoder\u001b[0m)               │ ?                      │       \u001b[38;5;34m173,198\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ discriminator (\u001b[38;5;33mDiscriminator\u001b[0m)   │ ?                      │        \u001b[38;5;34m38,401\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">386,083</span> (1.47 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m386,083\u001b[0m (1.47 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">385,315</span> (1.47 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m385,315\u001b[0m (1.47 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> (3.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m768\u001b[0m (3.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "True\n",
      "entered\n",
      "Trainable variables in the model:\n",
      "kernel\n",
      "bias\n",
      "kernel\n",
      "bias\n",
      "kernel\n",
      "bias\n",
      "kernel\n",
      "bias\n",
      "kernel\n",
      "bias\n",
      "kernel\n",
      "bias\n",
      "kernel\n",
      "bias\n",
      "kernel\n",
      "bias\n",
      "kernel\n",
      "bias\n",
      "kernel\n",
      "bias\n",
      "gamma\n",
      "beta\n",
      "kernel\n",
      "bias\n",
      "gamma\n",
      "beta\n",
      "kernel\n",
      "bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1.          0.          1.         ... -0.56966335 -0.9285436\n",
      "  -0.8050017 ]\n",
      " [ 1.          0.          0.         ... -0.56966335  2.5296998\n",
      "  -0.8050017 ]\n",
      " [ 1.          0.          0.         ... -0.56966335  0.22420415\n",
      "   0.3556489 ]\n",
      " ...\n",
      " [ 1.          1.          0.         ... -0.56966335  1.3769519\n",
      "  -0.8050017 ]\n",
      " [ 1.          0.          1.         ...  1.4205067  -0.9285436\n",
      "  -0.8050017 ]\n",
      " [ 1.          0.          0.         ... -0.56966335 -0.9285436\n",
      "  -0.8050017 ]], shape=(128, 14), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 1.          0.          1.         ... -0.56966335 -0.9285436\n",
      "  -0.8050017 ]\n",
      " [ 1.          0.          0.         ... -0.56966335  2.5296998\n",
      "  -0.8050017 ]\n",
      " [ 1.          0.          0.         ... -0.56966335  0.22420415\n",
      "   0.3556489 ]\n",
      " ...\n",
      " [ 1.          1.          0.         ... -0.56966335  1.3769519\n",
      "  -0.8050017 ]\n",
      " [ 1.          0.          1.         ...  1.4205067  -0.9285436\n",
      "  -0.8050017 ]\n",
      " [ 1.          0.          0.         ... -0.56966335 -0.9285436\n",
      "  -0.8050017 ]], shape=(128, 14), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200:   0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 33\u001b[0m\n\u001b[1;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39mdiscriminator\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     31\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/GPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlist_physical_devices(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/CPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 33\u001b[0m \u001b[43mtrain_vae_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 46\u001b[0m, in \u001b[0;36mtrain_vae_gan\u001b[0;34m(model, dataset, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     42\u001b[0m     gen_loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msigmoid_cross_entropy_with_logits(\n\u001b[1;32m     43\u001b[0m         logits\u001b[38;5;241m=\u001b[39mreal_or_fake_reconstructed, labels\u001b[38;5;241m=\u001b[39mreal_labels))\n\u001b[1;32m     45\u001b[0m gradients_dec \u001b[38;5;241m=\u001b[39m tape_dec\u001b[38;5;241m.\u001b[39mgradient(gen_loss, model\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[0;32m---> 46\u001b[0m \u001b[43moptimizer_dec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgradients_dec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Train VAE (Encoder + Decoder)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape_vae:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# Reconstruction Loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:344\u001b[0m, in \u001b[0;36mBaseOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[1;32m    343\u001b[0m     grads, trainable_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n\u001b[0;32m--> 344\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterations\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:397\u001b[0m, in \u001b[0;36mBaseOptimizer.apply\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    390\u001b[0m grads, trainable_variables \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_overwrite_variables_directly_with_gradients(\n\u001b[1;32m    392\u001b[0m         grads, trainable_variables\n\u001b[1;32m    393\u001b[0m     )\n\u001b[1;32m    394\u001b[0m )\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# Filter empty gradients.\u001b[39;00m\n\u001b[0;32m--> 397\u001b[0m grads, trainable_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter_empty_gradients\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(grads)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:733\u001b[0m, in \u001b[0;36mBaseOptimizer._filter_empty_gradients\u001b[0;34m(self, grads, vars)\u001b[0m\n\u001b[1;32m    730\u001b[0m             missing_grad_vars\u001b[38;5;241m.\u001b[39mappend(v\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filtered_grads:\n\u001b[0;32m--> 733\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo gradients provided for any variable.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_grad_vars:\n\u001b[1;32m    735\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    736\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradients do not exist for variables \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    737\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mreversed\u001b[39m(missing_grad_vars))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m when minimizing the loss.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    738\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m If using `model.compile()`, did you forget to provide a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    739\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`loss` argument?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    740\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming features_tensor is defined and dataset is provided\n",
    "input_dim = features_tensor.shape[1]\n",
    "\n",
    "\n",
    "encoder = Encoder(input_dim, latent_dim=10)\n",
    "decoder = Decoder(latent_dim=10, output_dim=input_dim)\n",
    "discriminator = Discriminator(input_dim)\n",
    "input_example = tf.random.normal([128, input_dim])  # Replace with your actual batch size and input dimension\n",
    "\n",
    "\n",
    "model = VAE_GAN(encoder, decoder, discriminator)\n",
    "mu,logvar = model.encoder(input_example)\n",
    "\n",
    "std = tf.exp(0.5 * logvar)\n",
    "eps = tf.random.normal(shape=tf.shape(std))\n",
    "encoder_output = mu + eps * std\n",
    "decoder_output = model.decoder(encoder_output)\n",
    "print(type(decoder_output))  # This should print <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
    "\n",
    "discriminator_output = model.discriminator(decoder_output)\n",
    "print(\"Encoder output shape:\", logvar.shape)\n",
    "print(\"Decoder output shape:\", decoder_output.shape)\n",
    "print(\"Discriminator output shape:\", discriminator_output[0].shape)\n",
    "print(model.summary())\n",
    "print(model.decoder.trainable)\n",
    "model.decoder.trainable = True\n",
    "model.encoder.trainable = True\n",
    "model.discriminator.trainable = True\n",
    "\n",
    "\n",
    "device = '/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'\n",
    "\n",
    "train_vae_gan(model, dataset, epochs=200, learning_rate=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
